setup:
  seed: 42
  experiment_name: keypoints
  architecture: HigherHRNet
  dataset: COCO
  run_name: null
  is_train: true
  ckpt_path: null
  pretrained_ckpt_path: pretrained/hrnet_32.pt
  deterministic: False

trainer:
  accelerator: gpu
  max_epochs: 300
  limit_batches: -1
  use_DDP: true
  sync_batchnorm: false
  use_compile: false

cudnn:
  benchmark: True
  deterministic: False
  enabled: True

dataloader:
  batch_size: 36
  pin_memory: true
  num_workers: 4
  train_ds:
    root: data/COCO
    split: train2017
    out_size: 512
    hm_resolutions: [0.25, 0.5]
    num_kpts: 17
    max_num_people: 30
    sigma: 2.0
    mosaic_probability: 0
  val_ds:
    root: data/COCO
    split: val2017
    out_size: 512
    hm_resolutions: [0.25, 0.5]
    num_kpts: 17
    max_num_people: 30
    sigma: 2.0
    mosaic_probability: 0

transform:
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]
  out_size: 512
  hm_resolutions: [0.25, 0.5]
  max_rotation: 30
  min_scale: 0.7
  max_scale: 1.6
  scale_type: short
  max_translate: 40

module:
  optimizers:
    optim:
      name: Adam
      params:
        lr: 0.001 # 0.0015 * ((2*36)/(4*12)) scale LR from paper by my batch_size (36) and GPU (2)
  lr_schedulers:
    optim:
      name: MultiStepLR
      interval: epoch
      params:
        milestones: [200, 260]
        gamma: 0.1

net:
  params:
    num_kpts: 17
    C: 32

inference:
  use_flip: False
  det_thr: 0.05
  tag_thr: 0.5
  input_size: 512
  ckpt_path: pretrained/higher_hrnet_32.pt
